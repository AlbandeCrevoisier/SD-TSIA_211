{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Sytèmes de recommandation\n",
    "\n",
    "\n",
    "## 1 Présentation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.optimize import brent\n",
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "\n",
    "#### Que fait l'option `minidata` de la fonction `load_movielens` de `movielensutils.py` ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movielens(filename, minidata=False):\n",
    "    \"\"\"\n",
    "    Cette fonction lit le fichier filename de la base de donnees\n",
    "    Movielens, par exemple \n",
    "    filename = '~/datasets/ml-100k/u.data'\n",
    "    Elle retourne \n",
    "    R : une matrice utilisateur-item contenant les scores\n",
    "    mask : une matrice valant 1 si il y a un score et 0 sinon\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename, dtype=int)\n",
    "\n",
    "    R = sparse.coo_matrix((data[:, 2], (data[:, 0]-1, data[:, 1]-1)),\n",
    "                          dtype=float)\n",
    "    R = R.toarray()  # not optimized for big data\n",
    "\n",
    "    # code la fonction 1_K\n",
    "    mask = sparse.coo_matrix((np.ones(data[:, 2].shape),\n",
    "                              (data[:, 0]-1, data[:, 1]-1)), dtype=bool)\n",
    "    mask = mask.toarray()  # not optimized for big data\n",
    "\n",
    "    if minidata is True:\n",
    "        R = R[0:100, 0:200].copy()\n",
    "        mask = mask[0:100, 0:200].copy()\n",
    "\n",
    "    return R, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'option `minidata` réduit les données de travail aux notes des 100 premiers utilisateurs pour les 200 premiers films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "\n",
    "#### Combien y a-t-il d'utilisateurs, de films référencés dans la base de données ? Quel est le nombre total de notes ?\n",
    "\n",
    "Il y a :\n",
    "* 943 utilisateurs : nombre de lignes de R\n",
    "* 1682 films : nombre de colonnes de R\n",
    "* 100 000 notes : donné par `print(np.sum(m_mask))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "\n",
    "#### La fonction objectif est-elle convexe ?\n",
    "\n",
    "Soit $f$ la fonction objectif : \n",
    "$\n",
    "f: (P, Q) \\mapsto \\frac{1}{2}\\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2 + \\frac{\\rho}{2}\\|P\\|_F^2 + \\frac{\\rho}{2}\\|Q\\|_F^2\n",
    "$.\n",
    "On remarque que $f$ est une forme quadratique.\n",
    "\n",
    "Fixons $\\rho = 0$ et $|U| = |C| = |I| = 1$. $f$ s'écrit alors : $f(p, q) = \\frac{1}{2}(r - qp)^2$.\n",
    "\n",
    "Soit $H(f)$ la matrice hessienne de f :\n",
    "$\n",
    "H(f) =\n",
    "\\begin{bmatrix}\n",
    "q^2 & 2qp -r \\\\\n",
    "2qp -r & p^2\n",
    "\\end{bmatrix}\n",
    "$\n",
    ", donc : $det(H(f)) = p^2q^2 - (2pq - r)^2 = (r - pq)(3pq - r)$ .\n",
    "\n",
    "Pour $p = q = 0$ et $r = 1$, $det(H(f)) = -1$, la hessienne n'est donc pas définie positive, $f$ n'est donc pas convexe.\n",
    "\n",
    "#### Quel est son gradient ?\n",
    "\n",
    "$f(P, Q) = \\frac{1}{2}\\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2 + \\frac{\\rho}{2}\\|P\\|_F^2 + \\frac{\\rho}{2}\\|Q\\|_F^2 = f_1(P,Q) + f_2(P) + f_3(Q)$\n",
    "\n",
    "$\\nabla{f}_Q$ :\n",
    "\n",
    "$\n",
    "f_1(P+H, Q)-f_1(P, Q) = \\frac{1}{2}(\\|\\mathbb{1}_K \\circ (R-Q(P+H))\\|_F^2 - \\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2)\n",
    "$  \n",
    "$\n",
    "= \\frac{1}{2}(\\|\\mathbb{1}_K \\circ (R-QP) - \\mathbb{1}_K \\circ QH)\\|_F^2 - \\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2)\n",
    "$  \n",
    "$\n",
    "= \\frac{1}{2}(\\|\\mathbb{1}_K \\circ (R-QP)\\|_F^2 + \\|\\mathbb{1}_K \\circ QH)\\|_F^2 \n",
    "-2<\\mathbb{1}_K \\circ (R-QP),\\mathbb{1}_K \\circ QH>\n",
    "- \\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2)\n",
    "$  \n",
    "$\n",
    "= - <Q^T(\\mathbb{1}_K \\circ (R-QP)), H> + o(H)\n",
    "$\n",
    "\n",
    "Donc $\\nabla{f}_Q(P, Q) = - Q^T(\\mathbb{1}_K \\circ (R-QP)) + \\rho P$.\n",
    "\n",
    "$\\nabla{f}_P$ :\n",
    "\n",
    "$\n",
    "f_1(P, Q+H)-f_1(P, Q) = \\frac{1}{2}(\\|\\mathbb{1}_K \\circ (R-(Q+H)P))\\|_F^2 - \\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2)\n",
    "$  \n",
    "$\n",
    "= \\frac{1}{2}(\\|\\mathbb{1}_K \\circ (R-QP) - \\mathbb{1}_K \\circ HP)\\|_F^2 - \\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2)\n",
    "$  \n",
    "$\n",
    "= \\frac{1}{2}(\\|\\mathbb{1}_K \\circ (R-QP)\\|_F^2 + \\|\\mathbb{1}_K \\circ HP)\\|_F^2 \n",
    "-2<\\mathbb{1}_K \\circ (R-QP),\\mathbb{1}_K \\circ HP>\n",
    "- \\|\\mathbb{1}_K \\circ (R - QP)\\|_F^2)\n",
    "$  \n",
    "$\n",
    "= - <(\\mathbb{1}_K \\circ (R-QP))P^T, H> + o(H)\n",
    "$\n",
    "\n",
    "Donc $\\nabla{f}_P(P, Q) = - (\\mathbb{1}_K \\circ (R-QP))P^T + \\rho P$.\n",
    "\n",
    "Donc :\n",
    "\n",
    "$$\n",
    "\\nabla{f}(P,Q) =\n",
    "\\begin{bmatrix}\n",
    "\\nabla{f_Q} \\\\\n",
    "\\nabla{f_P}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-Q^T(\\mathbb{1}_K \\circ (R - QP)) + \\rho P \\\\\n",
    "(\\mathbb{1}_K \\circ (R-QP))(-P^T) + \\rho Q\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "#### Est-il lipschitzien ? Donner la constance de Lipschitz le cas échéant.\n",
    "\n",
    "Si le gradient était Lipschitzien la hessienne serait bornée, or ici la hessienne est un polynôme d'ordre 2, $f$ n'est donc pas de gradient Lipschitzien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Trouver $P$ quand $Q_0$ est fixé\n",
    "$$\n",
    "g: P \\mapsto \\frac{1}{2}\\|\\mathbb{1}_K \\circ (R - Q^0P)\\|_F^2 + \\frac{\\rho}{2}\\|P\\|_F^2 + \\frac{\\rho}{2}\\|Q^0\\|_F^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "#### La fonction objectif $g$ est-elle convexe ?\n",
    "\n",
    "La hessienne $\\nabla^2{g}(P) = (Q^0)^TQ^0 + \\rho I$ est définie positive pour $\\rho$ convenablement choisi, g est donc convexe.\n",
    "\n",
    "### Quel est son gradient ?\n",
    "\n",
    "$$\n",
    "\\nabla{g}(P) = -(Q^0)^T(\\mathbb{1}_K \\circ (R-Q^0P)) + \\rho P\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(P, Q0, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme simplifie.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q0 : une matrice de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q0.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q0 ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = -Q0.transpose().dot(tmp) + rho*P\n",
    "\n",
    "    return val, grad_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification de l'implémentation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1221547542271297"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R, mask = load_movielens('ml-100k/u.data')\n",
    "Q0, s, P0 = svds(R, k=4)\n",
    "rho = 0.3\n",
    "check_grad(\n",
    "    lambda p: objective(np.reshape(p, np.shape(P0)), Q0, R, mask, rho)[0],\n",
    "    lambda p: np.ravel(objective(np.reshape(p, np.shape(P0)), Q0, R, mask, rho)[1]),\n",
    "    np.ravel(P0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur étant négligeable, l'implémentation est satisfaisante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(g, P0, gamma, epsilon):\n",
    "    \"\"\"\n",
    "    Minimise g par la méthode du gradient a pas constant.\n",
    "    Prend en entree\n",
    "    g : fonction a minimiser, Pk -> g(Pk), grad_g(Pk)\n",
    "    P0 : point de depart\n",
    "    gamma : pas constant\n",
    "    epsilon : critere d'arret, norme de Frobenius du gradient de g en Pk\n",
    "              inferieur ou egal a epsilon\n",
    "    \n",
    "    Sorties :\n",
    "    Pk : minimiseur\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_G = g(P0)[1]\n",
    "    Pk = P0\n",
    "    while np.sum(grad_G**2) > epsilon:\n",
    "        Pk = Pk - gamma*grad_G\n",
    "        grad_G = g(Pk)[1]\n",
    "    \n",
    "    return Pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "\n",
    "$\\epsilon = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.23195586e+00,   4.90901655e-01,   9.99038902e+00, ...,\n",
       "         -4.74720062e-01,   3.92005611e-02,   8.67313639e-01],\n",
       "       [  4.53690158e+00,  -1.35383903e+01,  -2.56918810e+00, ...,\n",
       "          3.63328948e-01,  -3.15228411e-01,  -1.77354579e-01],\n",
       "       [ -2.02973198e+01,  -4.69295419e-01,  -1.07439139e+01, ...,\n",
       "         -3.43309414e-01,   8.20572197e-02,   1.54910736e-01],\n",
       "       [  5.76413523e+01,   2.77434992e+01,   1.98610680e+01, ...,\n",
       "          6.08854704e-02,   6.75456877e-01,   6.32182974e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1\n",
    "L = rho + np.sum(Q0.transpose().dot(Q0)**2)\n",
    "gradient(lambda P: objective(P, Q0, R, mask, rho), P0, 1/L, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Raffinements algorithmiques pour le problème à $Q_0$ fixé\n",
    "\n",
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_linear(g, P0, epsilon):\n",
    "    \"\"\"\n",
    "    Minimise g par la methode du gradient avec recherche lineaire.\n",
    "    Prend en entree\n",
    "    g : fonction a minimiser, Pk -> g(Pk), grad_g(Pk)\n",
    "    P0 : point de depart\n",
    "    epsilon : critere d'arret, norme de Frobenius du gradient de g en Pk inferieur ou egal a epsilon\n",
    "\n",
    "    Sorties :\n",
    "    Pk : minimiseur\n",
    "    \"\"\"\n",
    "\n",
    "    a, b, beta = 1/2, 1/2, 10**(-4) # Armijo's linear search parameters\n",
    "\n",
    "    grad_G = g(P0)[1]\n",
    "    Pk = P0\n",
    "    while norm(grad_G) > epsilon:\n",
    "        # Armijo's linear search\n",
    "        l = 0\n",
    "        while g(Pk-b*(a**l)*grad_G)[0] > g(Pk)[0] - beta*b*(a**l)*norm(grad_G):\n",
    "            l = l + 1\n",
    "        Pk = Pk - b*(a**l)*grad_G\n",
    "        b = 2*b*(a**l)\n",
    "        grad_G = g(Pk)[1]\n",
    "\n",
    "    return Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.23867019e+00,   4.88013348e-01,   9.97161501e+00, ...,\n",
       "         -4.81058941e-01,   3.97404674e-02,   8.78795758e-01],\n",
       "       [  4.53543200e+00,  -1.35229851e+01,  -2.55365465e+00, ...,\n",
       "          3.68183224e-01,  -3.19572407e-01,  -1.79703893e-01],\n",
       "       [ -2.03074942e+01,  -4.68161609e-01,  -1.07548738e+01, ...,\n",
       "         -3.47897006e-01,   8.31881918e-02,   1.56963107e-01],\n",
       "       [  5.76592766e+01,   2.77584578e+01,   1.98915264e+01, ...,\n",
       "          6.16997703e-02,   6.84774112e-01,   6.40565861e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_linear(lambda P: objective(P, Q0, R, mask, rho), P0, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que $g$ est une forme quadradique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_conjugate(g, P0, epsilon):\n",
    "    \"\"\"\n",
    "    Minimise g par la methode du gradient conjugue : Fletcher et Reeves.\n",
    "    Prend en entree\n",
    "    g : fonction a minimiser, Pk -> g(Pk), grad_g(Pk)\n",
    "    P0 : point de depart\n",
    "    \n",
    "    Sorties :\n",
    "    Pk : minimiseur\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_G = g(P0)[1]\n",
    "    d = -grad_G\n",
    "    Pk = P0\n",
    "    while np.linalg.norm(grad_G) > epsilon:\n",
    "        s = brent(lambda s: g(Pk + s*d)[0])\n",
    "        Pk = Pk + s*d\n",
    "        prev_grad = grad_G\n",
    "        grad_G = g(Pk)[1]\n",
    "        b = np.sum(grad_G**2) / np.sum(prev_grad**2)\n",
    "        d = -grad_G + b*d\n",
    "\n",
    "        return Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.81549597e+00,   5.65741396e-01,   8.52713169e+00, ...,\n",
       "         -1.66842642e-01,   1.35359674e-02,   3.06240082e-01],\n",
       "       [ -4.03630355e+00,  -1.49313803e+01,  -2.78188027e+00, ...,\n",
       "          1.27563596e-01,  -1.08737554e-01,  -6.25583912e-02],\n",
       "       [ -2.33915218e+01,  -1.88169409e+00,  -7.68556846e+00, ...,\n",
       "         -1.20498054e-01,   2.82968797e-02,   5.46251021e-02],\n",
       "       [  6.73205130e+01,   2.46845268e+01,   1.39863834e+01, ...,\n",
       "          2.13376695e-02,   2.32572437e-01,   2.22583301e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_conjugate(lambda p: objective(p, Q0, R, mask, rho), P0, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "Mesure des performances des algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1569035449974763\n",
      "0.842063488002168\n",
      "0.2907360479985073\n"
     ]
    }
   ],
   "source": [
    "print(timeit('gradient(lambda P: objective(P, Q0, R, mask, rho), P0, 1/L, epsilon)', number=1, globals=globals()))\n",
    "print(timeit('gradient_linear(lambda P: objective(P, Q0, R, mask, rho), P0, epsilon)', number=1, globals=globals()))\n",
    "print(timeit('gradient_conjugate(lambda p: objective(p, Q0, R, mask, rho), P0, epsilon)', number=1, globals=globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Résolution du problème complet\n",
    "\n",
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_objective(P, Q, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme complet.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q : la variable matricielle de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    grad_Q : le gradient par rapport a Q\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = rho * P - Q.T.dot(tmp)\n",
    "\n",
    "    grad_Q = rho * Q - tmp.dot(P.T)\n",
    "\n",
    "    return val, grad_P, grad_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def rechercheLineaire(g,P0,Q0,epsilon):\n",
    "    P=P0\n",
    "    Q=Q0\n",
    "    val, grad_P,grad_Q = total_objective(P=P, Q=Q, R=R , mask=mask, rho=0.3)\n",
    "    norm_grad = np.sqrt(np.sum(grad_P**2)+np.sum(grad_Q**2))\n",
    "    while (norm_grad > epsilon):\n",
    "        l=0\n",
    "        a,b,beta=(0.5,2.0,0.5)\n",
    "        gamma=b\n",
    "        Pprime = P - gamma*grad_P\n",
    "        Qprime = Q - gamma*grad_Q\n",
    "        valprime, grad_Pprime,grad_Qprime= total_objective(P=Pprime, Q=Qprime, R=R , mask=mask, rho=0.3)\n",
    "        num=val+ beta*(np.trace(grad_P.T.dot(Pprime-P))+np.trace(grad_Q.T.dot(Qprime-Q)))\n",
    "        while(valprime> num):\n",
    "            l=l+1\n",
    "            gamma=b*a**l\n",
    "            Pprime = P - gamma*grad_P\n",
    "            Qprime = Q - gamma*grad_Q\n",
    "            valprime, grad_Pprime,grad_Qprime= g(P=Pprime, Q=Qprime, R=R , mask=mask, rho=0.3)\n",
    "            num=val+ beta*(np.trace(grad_P.T.dot(Pprime-P))+np.trace(grad_Q.T.dot(Qprime-Q)))\n",
    "        P=Pprime\n",
    "        Q=Qprime\n",
    "       \n",
    "        grad_P=grad_Pprime\n",
    "        grad_Q=grad_Qprime\n",
    "        val=valprime\n",
    "        norm_grad = np.sqrt(np.sum(grad_P**2)+np.sum(grad_Q**2))\n",
    "    return val,P,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35970.7267457\n"
     ]
    }
   ],
   "source": [
    "val,P,Q=rechercheLineaire(total_objective,P0,Q0,100)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur retournée par l'algorithme correspond a une estimation des notes que les utilisateurs mettraient au divers films qu'ils n'ont pas encore regardés( $R = QP$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquons que la fonction est convexe en chacune des variables mais n'est pas elle-même convexe. La méthode de résolution par la méthode des moindres carrés s'appuie sur la convexité en chacune des variables. En effet on est assuré d'avoir à chaque fois une valeur objectif qui décroît car à chaque fois que l'on avance dans les itérations, la minimisation de la fonction convexe par rapport à l'une ou l'autre des deux variables permet de trouver une valeur plus petite que précédement (cela est vrai car les deux variables ne varient pas en même temps).\n",
    "La fonction objectif étant minorée par 0 et décroissante , elle est donc convergente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientOnP(g,P0, Q0,epsilon, gamma):\n",
    "    P=P0\n",
    "    Q=Q0\n",
    "    val, grad_P, grad_Q = total_objective(P=P, Q=Q, R=R , mask=mask, rho=0.2)\n",
    "    grad = grad_P\n",
    "    norm_grad = np.sqrt(np.sum(grad ** 2))\n",
    "    while (norm_grad > epsilon) :\n",
    "        P = P - gamma*grad\n",
    "        val, grad_P, grad_Q = total_objective(P=P, Q=Q, R=R , mask=mask, rho=0.2)\n",
    "        grad=grad_P\n",
    "        norm_grad= np.sqrt(np.sum(grad_P ** 2))\n",
    "    return val, P\n",
    "\n",
    "def gradientOnQ(g,P0, Q0,epsilon, gamma):\n",
    "    P=P0\n",
    "    Q=Q0\n",
    "    val, grad_P, grad_Q = total_objective(P=P, Q=Q, R=R , mask=mask, rho=0.2)\n",
    "    grad = grad_Q\n",
    "    norm_grad = np.sqrt(np.sum(grad ** 2))\n",
    "    while (norm_grad > epsilon) :\n",
    "        Q = Q - gamma*grad\n",
    "        val, grad_P, grad_Q = total_objective(P=P, Q=Q, R=R , mask=mask, rho=0.2)\n",
    "        grad = grad_Q\n",
    "        norm_grad= np.sqrt(np.sum(grad_Q ** 2))\n",
    "    return val, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146828.165471\n",
      "80725.0327028\n",
      "77331.0604341\n"
     ]
    }
   ],
   "source": [
    "Q0, s, P0 = svds(R, k=4)\n",
    "Qb = Q0\n",
    "Pb = P0\n",
    "listeval = []\n",
    "n=10000\n",
    "\n",
    "while n > 100:\n",
    "    Qstor = Qb\n",
    "    val, Qb = gradientOnQ(total_objective,P0=Pb, Q0=Q0,epsilon=500, gamma=10**(-(len(listeval)+1)))\n",
    "    listeval.append(val)\n",
    "    Pstor = Pb\n",
    "    val, Pb = gradientOnP(total_objective,P0=P0, Q0=Qb,epsilon=5000, gamma=0.000001)\n",
    "    listeval.append(val)\n",
    "    n = np.sqrt(np.sum((Pstor - Pb) ** 2) + np.sum((Qstor - Qb)**2)) \n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La difference de valeur de R entre la méthode du gradient et la méthode des moindres carrés est:  1703.35031205\n",
      "La difference de valeur de P entre la méthode du gradient et la méthode des moindres carrés est:  74.4828602245\n",
      "La difference de valeur de Q entre la méthode du gradient et la méthode des moindres carrés est:  317.886471223\n"
     ]
    }
   ],
   "source": [
    "R1 = Q.dot(P)\n",
    "R2 = Qb.dot(Pb)\n",
    "norm = np.linalg.norm((R1-R2))\n",
    "norm2 = np.linalg.norm((Pb-P))\n",
    "norm3 = np.linalg.norm((Qb-Q))\n",
    "\n",
    "print(\"La difference de valeur de R entre la méthode du gradient et la méthode des moindres carrés est: \", norm)\n",
    "print(\"La difference de valeur de P entre la méthode du gradient et la méthode des moindres carrés est: \", norm2)\n",
    "print(\"La difference de valeur de Q entre la méthode du gradient et la méthode des moindres carrés est: \", norm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le film recommandé pour l'utilisateur 300 est : 312\n"
     ]
    }
   ],
   "source": [
    "newmask = np.logical_not(mask)\n",
    "newR = R1*newmask\n",
    "noteUser = newR[300,:].reshape(-1)\n",
    "print(\"Le film recommandé pour l'utilisateur 300 est :\", np.argmax(noteUser))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
